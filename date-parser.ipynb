{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mWarning: Empirical distributions on disk may perform slow because GNU DBM is not available. Please install and configure gdbm library for Python for better speed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pyprob\n",
    "from pyprob import Model\n",
    "import pyprob.distributions as dists\n",
    "\n",
    "import calendar\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHot2DCategorical(dists.Categorical):\n",
    "    def sample(self):\n",
    "        s = self._torch_dist.sample()\n",
    "        one_hot = self._probs * 0\n",
    "        for i, val in enumerate(s):\n",
    "            one_hot[i, int(val.item())] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def log_prob(self, x, *args, **kwargs):\n",
    "        # vector of one hot vectors\n",
    "        non_one_hot = torch.tensor([row.nonzero() for row in x])\n",
    "        return super().log_prob(non_one_hot, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateParser(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Date with Unkown Format\")\n",
    "        self.possible_dividers = ['\\\\', '/', '-', ' ', '_', ':', '.']\n",
    "        self.longest_string = len('31 / November / 2000')\n",
    "        self.all_symbols = list(string.ascii_uppercase) + \\\n",
    "                           [str(d) for d in range(10)] + \\\n",
    "                           self.possible_dividers + \\\n",
    "                           [' ']\n",
    "    def get_index(self, letter):\n",
    "        return self.all_symbols.index(letter)\n",
    "    def pad(self, date_string):\n",
    "        return date_string + ' ' * (self.longest_string - len(date_string))\n",
    "    def forward(self):\n",
    "        # all dates are between 0 AD and 4000 AD\n",
    "        # sanple each digit such that the year is usually close to 2019\n",
    "        year_1 = int(pyprob.sample(dists.Categorical(torch.tensor(\n",
    "            [0.05, 0.4, 0.4, 0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]\n",
    "        ))).item())\n",
    "        year_2 = int(pyprob.sample(dists.Categorical(torch.tensor(\n",
    "            [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.16, 0.7] if year_1 == 1 else\n",
    "            [0.7, 0.16, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02] if year_1 == 2 else\n",
    "            [0.1]*10\n",
    "        ))).item())\n",
    "        year_3 = int(pyprob.sample(dists.Categorical(torch.tensor([0.1]*10))).item())\n",
    "        year_4 = int(pyprob.sample(dists.Categorical(torch.tensor([0.1]*10))).item())\n",
    "        year = int(\"\".join(str(d) for d in [year_1, year_2, year_3, year_4]))\n",
    "        # sample month and day given the year\n",
    "        month = int(pyprob.sample(dists.Categorical(torch.tensor([1/12]*12))).item()) +1\n",
    "        if year == 0:\n",
    "            num_days = 31    # monthrange fails if year is 0\n",
    "        else:\n",
    "            num_days = calendar.monthrange(year, month)[1]             # number of days in this month\n",
    "        day_probs = [1/num_days]*num_days + [0.]*(31-num_days)     # probs of which day it is (in fixed length vector)\n",
    "        day = int(pyprob.sample(dists.Categorical(torch.tensor(day_probs))).item()) + 1\n",
    "        # sample format used to write day, month and year\n",
    "        yy = pyprob.sample(dists.Categorical(torch.tensor([0.5, 0.5]))).item()  # either yy or yyyy\n",
    "        m = pyprob.sample(dists.Categorical(torch.tensor([0.25]*4))).item()   # either m, mm or e.g. 'JAN'\n",
    "        d = pyprob.sample(dists.Categorical(torch.tensor([0.5, 0.5]))).item()   # either d or dd\n",
    "        real_date = {'day': day, 'month': month, 'year': year}\n",
    "        # put day, month and year in right format\n",
    "        if d:\n",
    "            day = str(day)\n",
    "        else:  # dd format\n",
    "            day = str(day).zfill(2)\n",
    "        # do month\n",
    "        if m == 0:\n",
    "            month = str(month)\n",
    "        elif m == 1:\n",
    "            month = str(month).zfill(2)\n",
    "        elif m == 2:\n",
    "            month = calendar.month_name[month]\n",
    "        else:\n",
    "            month = calendar.month_abbr[month]\n",
    "        # do year\n",
    "        if yy:\n",
    "            year = str(year).zfill(2)[-2:]\n",
    "        else:  # yyyy\n",
    "            year = str(year).zfill(4)\n",
    "        # sample order of day, month, year\n",
    "        # m/d/y or d/m/y or y/m/d (never y/d/m)\n",
    "        order = pyprob.sample(dists.Categorical(torch.tensor([1/3]*3))).item()\n",
    "        if order == 0:\n",
    "            date = [month, day, year]\n",
    "        elif order == 1:\n",
    "            date = [day, month, year]\n",
    "        else:\n",
    "            date = [year, month, day]\n",
    "        # select dividers\n",
    "        num_div = len(self.possible_dividers)\n",
    "        divider1 = int(pyprob.sample(dists.Categorical(torch.tensor([1/num_div]*num_div))).item())\n",
    "        divider2 = int(pyprob.sample(dists.Categorical(torch.tensor([1/num_div]*num_div))).item())\n",
    "        divider1 = self.possible_dividers[divider1]\n",
    "        divider2 = self.possible_dividers[divider2]\n",
    "        # sometimes put space before/after dividers\n",
    "        space1 = bool(pyprob.sample(dists.Categorical(torch.tensor([0.9, 0.1]))).item())\n",
    "        space2 = bool(pyprob.sample(dists.Categorical(torch.tensor([0.9, 0.1]))).item())\n",
    "        space3 = bool(pyprob.sample(dists.Categorical(torch.tensor([0.9, 0.1]))).item())\n",
    "        space4 = bool(pyprob.sample(dists.Categorical(torch.tensor([0.9, 0.1]))).item())\n",
    "        date =  \"\".join([date[0],\n",
    "                         ' ' if space1 else '',\n",
    "                         divider1,\n",
    "                         ' ' if space2 else '',\n",
    "                         date[1],\n",
    "                         ' ' if space3 else '',\n",
    "                         divider2,\n",
    "                         ' ' if space4 else '',\n",
    "                         date[2]]).upper()\n",
    "        # pad with spaces so tha number of observations is constant\n",
    "        padded_date = self.pad(date)\n",
    "        # make a categorical distribution that observes each letter independently (like 20 independent categoricals)\n",
    "        probs = torch.ones(self.longest_string, len(self.all_symbols))*0.001\n",
    "        for i, letter in enumerate(padded_date):\n",
    "            probs[i, self.get_index(letter)] = 1.\n",
    "        pyprob.observe(OneHot2DCategorical(probs),\n",
    "                       name=f\"date_string\")\n",
    "        \n",
    "        return date, real_date\n",
    "    def get_observes(self, date_string):\n",
    "        one_hot = torch.zeros(self.longest_string, len(self.all_symbols))\n",
    "        date_string = self.pad(date_string)\n",
    "        for i, letter in enumerate(date_string):\n",
    "            one_hot[i, self.get_index(letter)] = 1.\n",
    "        return {'date_string': one_hot}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new inference network...\n",
      "Observable date_string: observe embedding not specified, using the default FEEDFORWARD.\n",
      "Observable date_string: embedding depth not specified, using the default 2.\n",
      "Observe embedding dimension: 256\n",
      "Train. time | Epoch| Trace     | Init. loss| Min. loss | Curr. loss| T.since min | Traces/sec\n",
      "New layers, address: 40__forward__?__Categorical(len_probs:10)__1, distribution: Categorical\n",
      "New layers, address: 140__forward__?__Categorical(len_probs:10)__1, distribution: Categorical\n",
      "New layers, address: 176__forward__?__Categorical(len_probs:10)__1, distribution: Categorical\n",
      "New layers, address: 212__forward__?__Categorical(len_probs:10)__1, distribution: Categorical\n",
      "New layers, address: 280__forward__?__Categorical(len_probs:12)__1, distribution: Categorical\n",
      "New layers, address: 374__forward__?__Categorical(len_probs:31)__1, distribution: Categorical\n",
      "New layers, address: 410__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "New layers, address: 442__forward__?__Categorical(len_probs:4)__1, distribution: Categorical\n",
      "New layers, address: 472__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "New layers, address: 668__forward__?__Categorical(len_probs:3)__1, distribution: Categorical\n",
      "New layers, address: 770__forward__?__Categorical(len_probs:7)__1, distribution: Categorical\n",
      "New layers, address: 810__forward__?__Categorical(len_probs:7)__1, distribution: Categorical\n",
      "New layers, address: 864__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "New layers, address: 898__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "New layers, address: 932__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "New layers, address: 966__forward__?__Categorical(len_probs:2)__1, distribution: Categorical\n",
      "Total addresses: 16, distribution types: 1, parameters: 4,819,870\n",
      "0d:00:01:20 | 1    | 10,048    | +2.58e+01 | +2.11e+01 | \u001b[31m+2.14e+01\u001b[0m | 0d:00:00:01 | 117.0                              \n"
     ]
    }
   ],
   "source": [
    "model = DateParser()\n",
    "model.learn_inference_network(\n",
    "    inference_network=pyprob.InferenceNetwork.LSTM,\n",
    "    observe_embeddings={'date_string': {'dim' : 256}},\n",
    "    num_traces=1000,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent  | Time remain.| Progress             | Trace | Traces/sec\n",
      "0d:00:00:01 | 0d:00:00:00 | #################### | 50/50 | 35.46       \n",
      "('14_DECEMBER-1504', {'day': 14, 'month': 12, 'year': 1504})\n",
      "('14_DECEMBER-1504', {'day': 14, 'month': 12, 'year': 1504})\n",
      "('14_DECEMBER-1504', {'day': 14, 'month': 12, 'year': 1504})\n"
     ]
    }
   ],
   "source": [
    "post = model.posterior_distribution(\n",
    "    observe=model.get_observes('16:DECEMBER 1944'),\n",
    "    inference_engine=pyprob.InferenceEngine.IMPORTANCE_SAMPLING_WITH_INFERENCE_NETWORK,\n",
    "    num_traces=50\n",
    ")\n",
    "print(post.sample())\n",
    "print(post.sample())\n",
    "print(post.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
